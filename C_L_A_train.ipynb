{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "g:\\dev\\.anaconda\\Miniconda\\envs\\py36_tf19\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "g:\\dev\\.anaconda\\Miniconda\\envs\\py36_tf19\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "g:\\dev\\.anaconda\\Miniconda\\envs\\py36_tf19\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "g:\\dev\\.anaconda\\Miniconda\\envs\\py36_tf19\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "g:\\dev\\.anaconda\\Miniconda\\envs\\py36_tf19\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "g:\\dev\\.anaconda\\Miniconda\\envs\\py36_tf19\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print('shapeä¸º',layer_activations.shape)\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def get_data(n, input_dim, attention_column=1):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column] = y[:, 0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network    should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data_recurrent2(n, time_steps, input_dim, attention_dim=5):\n",
    "    \"\"\"\n",
    "    Suppose input_dim = 10  time_steps = 6\n",
    "    formed one  x 6 x 10 The data of each step 6 dimension is the same as y\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:,:,attention_dim] =  np.tile(y[:], (1, time_steps))\n",
    "\n",
    "\n",
    "    return x,y\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.layers.merging'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-388efa84b755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.merging'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, LSTM ,Conv1D,Dropout,Bidirectional,Multiply\n",
    "from keras.models import Model\n",
    "\n",
    "#from attention_utils import get_activations\n",
    "from keras.layers import Multiply\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "\n",
    "import  pandas as pd\n",
    "import  numpy as np\n",
    "\n",
    "from keras.layers.merging.concatenate import concatenate\n",
    "from keras.layers.merging.add import add\n",
    "from keras.layers.merging.multiply import multiply\n",
    "from keras.layers.merging.subtract import subtract\n",
    "from keras.layers.merging.average import average\n",
    "from keras.layers.merging.maximum import maximum\n",
    "from keras.layers.merging.minimum import minimum\n",
    "from keras.layers.merging.dot import dot\n",
    "\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "# Another way of writing the attention mechanism is suitable for the use of the above error source:https://blog.csdn.net/uhauha2929/article/details/80733255\n",
    "def attention_3d_block2(inputs, single_attention_vector=False):\n",
    "    # If the upper layer is LSTM, you need return_sequences=True\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    time_steps = K.int_shape(inputs)[1]\n",
    "    input_dim = K.int_shape(inputs)[2]\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # Multiplied by the attention weight, but there is no summation, it seems to have little effect\n",
    "    # If you classify tasks, you can do Flatten expansion\n",
    "    # element-wise\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back):\n",
    "    '''\n",
    "    Processing the data\n",
    "    '''\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),1:]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back,:])\n",
    "    TrainX = np.array(dataX)\n",
    "    Train_Y = np.array(dataY)\n",
    "\n",
    "    return TrainX, Train_Y\n",
    "\n",
    "# Multidimensional normalization returns data and maximum and minimum values\n",
    "def NormalizeMult(data):\n",
    "    #normalize Used for denormalization\n",
    "    data = np.array(data)\n",
    "    normalize = np.arange(2*data.shape[1],dtype='float64')\n",
    "\n",
    "    normalize = normalize.reshape(data.shape[1],2)\n",
    "    print(normalize.shape)\n",
    "    for i in range(0,data.shape[1]):\n",
    "        #Column i\n",
    "        list = data[:,i]\n",
    "        listlow,listhigh =  np.percentile(list, [0, 100])\n",
    "        # print(i)\n",
    "        normalize[i,0] = listlow\n",
    "        normalize[i,1] = listhigh\n",
    "        delta = listhigh - listlow\n",
    "        if delta != 0:\n",
    "            #Row j\n",
    "            for j in range(0,data.shape[0]):\n",
    "                data[j,i]  =  (data[j,i] - listlow)/delta\n",
    "    #np.save(\"./normalize.npy\",normalize)\n",
    "    return  data,normalize\n",
    "\n",
    "# Multidimensional denormalization\n",
    "def FNormalizeMult(data,normalize):\n",
    "    data = np.array(data)\n",
    "    for i in  range(0,data.shape[1]):\n",
    "        listlow =  normalize[i,0]\n",
    "        listhigh = normalize[i,1]\n",
    "        delta = listhigh - listlow\n",
    "        if delta != 0:\n",
    "            #Row j\n",
    "            for j in range(0,data.shape[0]):\n",
    "                data[j,i]  =  data[j,i]*delta + listlow\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def attention_model():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))\n",
    "\n",
    "    x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    x = Dropout(drop)(x)\n",
    "\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    #For GPU you can use CuDNNLSTM\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    lstm_out = Dropout(drop)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "    #output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    output = Dense(1, activation='linear')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"P_2S_700K_1000K.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(legend=True, subplots=True, figsize=(12,8), xlabel=\"STEP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['time'], axis = 1)\n",
    "print(data.columns)\n",
    "print(data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¹åèªå±æ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIMS = 13\n",
    "TIME_STEPS = 50\n",
    "lstm_units = 64\n",
    "drop = 0\n",
    "\n",
    "#Normalized\n",
    "data,normalize = NormalizeMult(data)\n",
    "pollution_data = data[:,0].reshape(len(data),1)\n",
    "\n",
    "train_X, _ = create_dataset(data,TIME_STEPS)\n",
    "_ , train_Y = create_dataset(pollution_data,TIME_STEPS)\n",
    "\n",
    "print(train_X.shape,train_Y.shape)\n",
    "\n",
    "m = attention_model()\n",
    "m.summary()\n",
    "m.compile(optimizer='adam', loss='mse')\n",
    "#m.fit([train_X], train_Y, epochs=10, batch_size=64, validation_split=0.1)\n",
    "#m.save(\"./model.h5\")\n",
    "#np.save(\"normalize.npy\",normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"P_2S_700K_1000K.csv\")\n",
    "df = df.drop(['time'], axis = 1)\n",
    "train_size = int(len(df)*0.9)#æ°æ®åå\n",
    "train = df.iloc[:train_size,:]\n",
    "test = df.iloc[train_size:,:]\n",
    "#train, test = train_test_split(df, test_size=0.1)\n",
    "print(\"len(train):\",len(train))\n",
    "print(\"len(test):\",len(test))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "model_path = './model.h5'\n",
    "# TRAIN\n",
    "INPUT_DIMS = 11\n",
    "TIME_STEPS = 50\n",
    "lstm_units = 64\n",
    "\n",
    "#Normalized\n",
    "train,normalize = NormalizeMult(train)\n",
    "pollution_data = train[:,0].reshape(len(train),1)\n",
    "\n",
    "train_X, _ = create_dataset(train,TIME_STEPS)\n",
    "_ , train_Y = create_dataset(pollution_data,TIME_STEPS)\n",
    "print(train_X.shape,train_Y.shape)\n",
    "\n",
    "m = attention_model()\n",
    "m.summary()\n",
    "#m.compile(optimizer='adam', loss='mse')\n",
    "m.compile(loss='mae', optimizer='adam',metrics=['mae',r2_keras])\n",
    "\n",
    "# fit the network\n",
    "history =  m.fit([train_X], train_Y, epochs=30, batch_size=30, validation_split=0.1)\n",
    "#history = m.fit([train_X], train_Y, epochs=10, batch_size=64, validation_split=0.05, verbose=2,\n",
    "#          callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "#                       ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "#          )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"./C_L_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for R^2\n",
    "plt.plot(history.history['r2_keras'])\n",
    "plt.plot(history.history['val_r2_keras'])\n",
    "plt.title('model r^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "train,normalize = NormalizeMult(train)\n",
    "pollution_test = train[:,0].reshape(len(train),1)\n",
    "\n",
    "test_X, _ = create_dataset(train,TIME_STEPS)\n",
    "_ , test_Y = create_dataset(pollution_test,TIME_STEPS)\n",
    "print(test_X.shape,test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction\n",
    "scores_test = m.evaluate([test_X], test_Y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = m.predict([test_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_acc = plt.figure(figsize=(9, 9))\n",
    "plt.plot(results[0:27000])\n",
    "plt.plot(test_Y[0:27000])\n",
    "plt.title('real vs pred')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('epoch')\n",
    "plt.xlim((19000, 23000))\n",
    "plt.legend(['pred', 'real'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('ä»¥ä¸æ¯CNN_biLSTM_Attentionè¯¯å·®')\n",
    "print('R^2å³å®ç³»æ°ï¼',r2_score(test_Y[000:14000],results[00:14000]))\n",
    "print('RMSEä¸ºï¼',np.sqrt(mean_squared_error(test_Y[2500:18000],results[2500:18000])))\n",
    "print('MAPEä¸ºï¼',(abs(results[9000:15000] -test_Y[9000:15000])/ test_Y[9000:15000]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new=[]\n",
    "predict_new=[] \n",
    "\n",
    "for k in range(len(results)):\n",
    "    if test_Y[k]!=0:\n",
    "        test_new.append(test_Y[k])\n",
    "        predict_new.append(results[k])\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.fabs((y_true - y_pred) / y_true))\n",
    "mape = format(MAPE(test_new, predict_new), '.4f') \n",
    "mape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
