{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1cb6eb59ca1c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f4d3394a9e7d"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
        "    # Documentation is available online on Github at the address below.\n",
        "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
        "    print('----- activations -----')\n",
        "    activations = []\n",
        "    inp = model.input\n",
        "    if layer_name is None:\n",
        "        outputs = [layer.output for layer in model.layers]\n",
        "    else:\n",
        "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
        "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
        "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
        "    for layer_activations in layer_outputs:\n",
        "        activations.append(layer_activations)\n",
        "        if print_shape_only:\n",
        "            print(layer_activations.shape)\n",
        "        else:\n",
        "            print('shape为',layer_activations.shape)\n",
        "            print(layer_activations)\n",
        "    return activations\n",
        "\n",
        "\n",
        "def get_data(n, input_dim, attention_column=1):\n",
        "    \"\"\"\n",
        "    Data generation. x is purely random except that it's first value equals the target y.\n",
        "    In practice, the network should learn that the target = x[attention_column].\n",
        "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
        "    :param n: the number of samples to retrieve.\n",
        "    :param input_dim: the number of dimensions of each element in the series.\n",
        "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
        "    :return: x: model inputs, y: model targets\n",
        "    \"\"\"\n",
        "    x = np.random.standard_normal(size=(n, input_dim))\n",
        "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
        "    x[:, attention_column] = y[:, 0]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
        "    \"\"\"\n",
        "    Data generation. x is purely random except that it's first value equals the target y.\n",
        "    In practice, the network    should learn that the target = x[attention_column].\n",
        "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
        "    :param n: the number of samples to retrieve.\n",
        "    :param time_steps: the number of time steps of your series.\n",
        "    :param input_dim: the number of dimensions of each element in the series.\n",
        "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
        "    :return: x: model inputs, y: model targets\n",
        "    \"\"\"\n",
        "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
        "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
        "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def get_data_recurrent2(n, time_steps, input_dim, attention_dim=5):\n",
        "    \"\"\"\n",
        "    Suppose input_dim = 10  time_steps = 6\n",
        "    formed one  x 6 x 10 The data of each step 6 dimension is the same as y\n",
        "    \"\"\"\n",
        "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
        "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
        "    x[:,:,attention_dim] =  np.tile(y[:], (1, time_steps))\n",
        "\n",
        "\n",
        "    return x,y\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e8df1dae1d93"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.merging'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 42\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, LSTM ,Conv1D,Dropout,Bidirectional,Multiply\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#from attention_utils import get_activations\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# from keras.layers.core import *\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcatenate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concatenate\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiply\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.merging'"
          ]
        }
      ],
      "source": [
        "# 本导入顺序可以看到类型\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import tensorflow_docs\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from attention_utils import get_activations\n",
        "\n",
        "from sklearnex import patch_sklearn\n",
        "from matplotlib import axis, pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import ensemble, metrics\n",
        "\n",
        "\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "# from keras.layers import (\n",
        "#     Input,\n",
        "#     Dense,\n",
        "#     LSTM,\n",
        "#     Conv1D,\n",
        "#     Dropout,\n",
        "#     Bidirectional,\n",
        "#     Multiply,\n",
        "#     Lambda,\n",
        "#     RepeatVector,\n",
        "#     Permute,\n",
        "# )\n",
        "\n",
        "\n",
        "def merging(inputs, axis):\n",
        "    return layers.concatenate(inputs, axis)\n",
        "\n",
        "\n",
        "# from attention_utils import get_activations\n",
        "# from keras.layers.core import *\n",
        "# from keras.layers.merging.concatenate import concatenate\n",
        "# from keras.layers.merging.add import add\n",
        "# from keras.layers.merging.multiply import multiply\n",
        "# from keras.layers.merging.subtract import subtract\n",
        "# from keras.layers.merging.average import average\n",
        "# from keras.layers.merging.maximum import maximum\n",
        "# from keras.layers.merging.minimum import minimum\n",
        "# from keras.layers.merging.dot import dot\n",
        "\n",
        "SINGLE_ATTENTION_VECTOR = False\n",
        "\n",
        "\n",
        "def attention_3d_block(inputs):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "\n",
        "    input_dim = int(inputs.shape[2])\n",
        "\n",
        "    a = inputs\n",
        "\n",
        "    # a = Permute((2, 1))(inputs)\n",
        "\n",
        "    # a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "\n",
        "    a = Dense(input_dim, activation=\"softmax\")(a)\n",
        "\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1), name=\"dim_reduction\")(a)\n",
        "\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "\n",
        "    a_probs = Permute((1, 2), name=\"attention_vec\")(a)\n",
        "\n",
        "    # output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
        "\n",
        "    output_attention_mul = Multiply()([inputs, a_probs])\n",
        "    return output_attention_mul\n",
        "\n",
        "\n",
        "# Another way of writing the attention mechanism is suitable for the use of the above error source:https://blog.csdn.net/uhauha2929/article/details/80733255\n",
        "\n",
        "\n",
        "def attention_3d_block2(inputs, single_attention_vector=False):\n",
        "    # If the upper layer is LSTM, you need return_sequences=True\n",
        "\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "\n",
        "    time_steps = K.int_shape(inputs)[1]\n",
        "\n",
        "    input_dim = K.int_shape(inputs)[2]\n",
        "\n",
        "    a = Permute((2, 1))(inputs)\n",
        "\n",
        "    a = Dense(time_steps, activation=\"softmax\")(a)\n",
        "\n",
        "    if single_attention_vector:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
        "\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "\n",
        "    a_probs = Permute((2, 1))(a)\n",
        "\n",
        "    # Multiplied by the attention weight, but there is no summation, it seems to have little effect\n",
        "\n",
        "    # If you classify tasks, you can do Flatten expansion\n",
        "\n",
        "    # element-wise\n",
        "\n",
        "    output_attention_mul = Multiply()([inputs, a_probs])\n",
        "    return output_attention_mul\n",
        "\n",
        "\n",
        "def create_dataset(dataset, look_back):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Processing the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    dataX, dataY = [], []\n",
        "\n",
        "    for i in range(len(dataset) - look_back - 1):\n",
        "        a = dataset[i : (i + look_back), 1:]\n",
        "\n",
        "        dataX.append(a)\n",
        "\n",
        "        dataY.append(dataset[i + look_back, :])\n",
        "\n",
        "    TrainX = np.array(dataX)\n",
        "\n",
        "    Train_Y = np.array(dataY)\n",
        "\n",
        "    return TrainX, Train_Y\n",
        "\n",
        "\n",
        "# Multidimensional normalization returns data and maximum and minimum values\n",
        "\n",
        "\n",
        "def NormalizeMult(data):\n",
        "    # normalize Used for denormalization\n",
        "\n",
        "    data = np.array(data)\n",
        "\n",
        "    normalize = np.arange(2 * data.shape[1], dtype=\"float64\")\n",
        "\n",
        "    normalize = normalize.reshape(data.shape[1], 2)\n",
        "\n",
        "    print(normalize.shape)\n",
        "\n",
        "    for i in range(0, data.shape[1]):\n",
        "        # Column i\n",
        "\n",
        "        list = data[:, i]\n",
        "\n",
        "        listlow, listhigh = np.percentile(list, [0, 100])\n",
        "\n",
        "        # print(i)\n",
        "\n",
        "        normalize[i, 0] = listlow\n",
        "\n",
        "        normalize[i, 1] = listhigh\n",
        "\n",
        "        delta = listhigh - listlow\n",
        "\n",
        "        if delta != 0:\n",
        "            # Row j\n",
        "\n",
        "            for j in range(0, data.shape[0]):\n",
        "                data[j, i] = (data[j, i] - listlow) / delta\n",
        "\n",
        "    # np.save(\"./normalize.npy\",normalize)\n",
        "\n",
        "    return data, normalize\n",
        "\n",
        "\n",
        "# Multidimensional denormalization\n",
        "\n",
        "\n",
        "def FNormalizeMult(data, normalize):\n",
        "    data = np.array(data)\n",
        "\n",
        "    for i in range(0, data.shape[1]):\n",
        "        listlow = normalize[i, 0]\n",
        "\n",
        "        listhigh = normalize[i, 1]\n",
        "\n",
        "        delta = listhigh - listlow\n",
        "\n",
        "        if delta != 0:\n",
        "            # Row j\n",
        "\n",
        "            for j in range(0, data.shape[0]):\n",
        "                data[j, i] = data[j, i] * delta + listlow\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def attention_model():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))\n",
        "\n",
        "    x = Conv1D(filters=64, kernel_size=1, activation=\"relu\")(\n",
        "        inputs\n",
        "    )  # , padding = 'same'\n",
        "\n",
        "    x = Dropout(drop)(x)\n",
        "\n",
        "    # lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
        "\n",
        "    # For GPU you can use CuDNNLSTM\n",
        "\n",
        "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
        "\n",
        "    lstm_out = Dropout(drop)(lstm_out)\n",
        "\n",
        "    attention_mul = attention_3d_block(lstm_out)\n",
        "\n",
        "    attention_mul = Flatten()(attention_mul)\n",
        "\n",
        "    # output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "\n",
        "    output = Dense(1, activation=\"linear\")(attention_mul)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=output)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50a0ebd97a0d"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"P_2S_700K_1000K.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22da71e0ea10"
      },
      "outputs": [],
      "source": [
        "data.plot(legend=True, subplots=True, figsize=(12,8), xlabel=\"STEP\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f545f9fac5d6"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['time'], axis = 1)\n",
        "print(data.columns)\n",
        "print(data.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e4780dd2a7"
      },
      "source": [
        "### 改各自层数！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3254d815e161"
      },
      "outputs": [],
      "source": [
        "INPUT_DIMS = 13\n",
        "TIME_STEPS = 50\n",
        "lstm_units = 64\n",
        "drop = 0\n",
        "\n",
        "#Normalized\n",
        "data,normalize = NormalizeMult(data)\n",
        "pollution_data = data[:,0].reshape(len(data),1)\n",
        "\n",
        "train_X, _ = create_dataset(data,TIME_STEPS)\n",
        "_ , train_Y = create_dataset(pollution_data,TIME_STEPS)\n",
        "\n",
        "print(train_X.shape,train_Y.shape)\n",
        "\n",
        "m = attention_model()\n",
        "m.summary()\n",
        "m.compile(optimizer='adam', loss='mse')\n",
        "#m.fit([train_X], train_Y, epochs=10, batch_size=64, validation_split=0.1)\n",
        "#m.save(\"./model.h5\")\n",
        "#np.save(\"normalize.npy\",normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a4581775686"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.read_csv(\"P_2S_700K_1000K.csv\")\n",
        "df = df.drop(['time'], axis = 1)\n",
        "train_size = int(len(df)*0.9)#数据划分\n",
        "train = df.iloc[:train_size,:]\n",
        "test = df.iloc[train_size:,:]\n",
        "#train, test = train_test_split(df, test_size=0.1)\n",
        "print(\"len(train):\",len(train))\n",
        "print(\"len(test):\",len(test))\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3e5a23b6009"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "def r2_keras(y_true, y_pred):\n",
        "    \"\"\"Coefficient of Determination \n",
        "    \"\"\"\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8610d9c2b8f3"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "model_path = './model.h5'\n",
        "# TRAIN\n",
        "INPUT_DIMS = 11\n",
        "TIME_STEPS = 50\n",
        "lstm_units = 64\n",
        "\n",
        "#Normalized\n",
        "train,normalize = NormalizeMult(train)\n",
        "pollution_data = train[:,0].reshape(len(train),1)\n",
        "\n",
        "train_X, _ = create_dataset(train,TIME_STEPS)\n",
        "_ , train_Y = create_dataset(pollution_data,TIME_STEPS)\n",
        "print(train_X.shape,train_Y.shape)\n",
        "\n",
        "m = attention_model()\n",
        "m.summary()\n",
        "#m.compile(optimizer='adam', loss='mse')\n",
        "m.compile(loss='mae', optimizer='adam',metrics=['mae',r2_keras])\n",
        "\n",
        "# fit the network\n",
        "history =  m.fit([train_X], train_Y, epochs=30, batch_size=30, validation_split=0.1)\n",
        "#history = m.fit([train_X], train_Y, epochs=10, batch_size=64, validation_split=0.05, verbose=2,\n",
        "#          callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "#                       ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
        "#          )\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c01cdff7f11a"
      },
      "outputs": [],
      "source": [
        "m.save(\"./C_L_A.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "014a053d5832"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for R^2\n",
        "plt.plot(history.history['r2_keras'])\n",
        "plt.plot(history.history['val_r2_keras'])\n",
        "plt.title('model r^2')\n",
        "plt.ylabel('R^2')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for Loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "233466ee024c"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "train,normalize = NormalizeMult(train)\n",
        "pollution_test = train[:,0].reshape(len(train),1)\n",
        "\n",
        "test_X, _ = create_dataset(train,TIME_STEPS)\n",
        "_ , test_Y = create_dataset(pollution_test,TIME_STEPS)\n",
        "print(test_X.shape,test_Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "199446f7bc01"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "scores_test = m.evaluate([test_X], test_Y, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af9362b79af9"
      },
      "outputs": [],
      "source": [
        "results = m.predict([test_X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0131a44913a6"
      },
      "outputs": [],
      "source": [
        "fig_acc = plt.figure(figsize=(9, 9))\n",
        "plt.plot(results[0:27000])\n",
        "plt.plot(test_Y[0:27000])\n",
        "plt.title('real vs pred')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('epoch')\n",
        "plt.xlim((19000, 23000))\n",
        "plt.legend(['pred', 'real'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68bcc83a780d"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "print('以下是CNN_biLSTM_Attention误差')\n",
        "print('R^2决定系数：',r2_score(test_Y[000:14000],results[00:14000]))\n",
        "print('RMSE为：',np.sqrt(mean_squared_error(test_Y[2500:18000],results[2500:18000])))\n",
        "print('MAPE为：',(abs(results[9000:15000] -test_Y[9000:15000])/ test_Y[9000:15000]).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "474f9c764c94"
      },
      "outputs": [],
      "source": [
        "test_new=[]\n",
        "predict_new=[] \n",
        "\n",
        "for k in range(len(results)):\n",
        "    if test_Y[k]!=0:\n",
        "        test_new.append(test_Y[k])\n",
        "        predict_new.append(results[k])\n",
        "\n",
        "def MAPE(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.fabs((y_true - y_pred) / y_true))\n",
        "mape = format(MAPE(test_new, predict_new), '.4f') \n",
        "mape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C_L_A_train.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
